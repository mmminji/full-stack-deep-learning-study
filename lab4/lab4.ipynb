{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!python training/run_experiment.py --max_epochs=40 --gpus=1 --num_workers=16 --data_class=EMNISTLines --min_overlap=0 --max_overlap=0.33 --model_class=LineCNNTransformer --window_width=20 --window_stride=12 --loss=transformer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EMNISTLinesDataset loading data from HDF5...\n",
      "EMNISTLinesDataset loading data from HDF5...\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.4228, device='cuda:0'),\n",
      " 'test_cer': tensor(0.5794, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "    | Name                                                       | Type                    | Params\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                      | LineCNNTransformer      | 3.8 M \n",
      "1   | model.line_cnn                                             | LineCNN                 | 1.1 M \n",
      "2   | model.line_cnn.convs                                       | Sequential              | 698 K \n",
      "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
      "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
      "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
      "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
      "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
      "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
      "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
      "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
      "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
      "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
      "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
      "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
      "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
      "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
      "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
      "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
      "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
      "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
      "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
      "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
      "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
      "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
      "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
      "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
      "27  | model.line_cnn.convs.8                                     | ConvBlock               | 393 K \n",
      "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 393 K \n",
      "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
      "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
      "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
      "32  | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
      "33  | model.embedding                                            | Embedding               | 21.2 K\n",
      "34  | model.fc                                                   | Linear                  | 21.3 K\n",
      "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
      "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
      "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.6 M \n",
      "38  | model.transformer_decoder.layers                           | ModuleList              | 2.6 M \n",
      "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
      "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
      "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
      "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
      "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
      "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
      "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
      "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
      "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
      "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
      "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
      "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
      "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
      "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
      "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
      "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
      "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
      "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
      "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
      "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
      "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
      "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
      "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
      "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
      "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
      "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
      "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
      "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
      "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
      "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
      "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
      "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
      "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
      "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
      "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
      "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
      "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
      "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
      "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
      "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
      "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
      "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
      "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
      "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
      "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
      "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
      "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
      "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
      "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
      "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
      "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
      "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
      "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
      "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
      "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
      "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
      "95  | train_acc                                                  | Accuracy                | 0     \n",
      "96  | val_acc                                                    | Accuracy                | 0     \n",
      "97  | test_acc                                                   | Accuracy                | 0     \n",
      "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
      "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
      "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "interpreter": {
   "hash": "eaa46ed45755c4c68b9b1cac48675e90783b9d9fa17c31df0eef0323bed0b7bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}